{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_Close Crawl.ipynb","provenance":[],"collapsed_sections":["1SXQJI6JlNRB"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ENAzMwrvUymM"},"source":["# default_exp closecrawl"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KuI-fu5UrvlK"},"source":["## Intro"]},{"cell_type":"markdown","metadata":{"id":"p9NYNpdsncE8"},"source":["# README from DataLibraries/BaltimoreCircuitCourt/META"]},{"cell_type":"code","metadata":{"id":"0hxkH351bQOP"},"source":["#export \n","casetype = 'O'\n","year = 20\n","lowerbound = 0000\n","upperbound = 9000\n","output = 'outputfile.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jIV5fihKm74w"},"source":["Instructions for obtaining foreclosure case data from MD Case Search with Web Scraper: (These instructions are for the scraper designed in 2017 forward)\n","________________________________________________________________________\n","\n","You can run it from Google Colabs. It goes CLI CLIARGS > MAIN> SPIDER > MINER > CLEANER\n","\n","The code in this ipynb should just run if you hit \"Runtime -> run all\" from within google colabs.\n","\n","You will probably want to enter your own parameters.\n","\n","Some details on the available parameters: \n","\n","1. Case type: C for tax sale foreclosures, O for mortgage foreclosures\n","> To calculate foreclosure we use mortgage foreclosures(O) as apposed to tax Foreclosures(C).\n","  We have only ever looked at tax foreclosures for specific projects like BOLD.\n","\n","\n","\n","2. Year: (Year must be 4 digits)\n","\n","3. Enter the lower bound for the cases to scrape (1-4 digit integer). \n","        A. If you are scraping cases from a year for the first time that year, you will want to start with \"1\". \n","        B. In all other scenarios you need to go look at the last data scraped in the raw folder of that dataset. Look at the case number for the last case that was scraped,              look at the last for digits and go up by one number. For example, if the last case scraped was case number 24O17002002, you will want to make the lower bound 2003. \n","\n","4. Enter the upper bound (1-4 digit integer): It is recommended to not try to scrape too many at a time, this sometimes results in errors. Scrape about 1500 at a time. The        upper bound should be about 1500 higher a number than your lower bound. \n","\tA. Sometimes it is difficult to know how far to scrape because you don't know how many total cases there are for a given year. One suggestion is to look at the                    dates of the cases you have scrapped. If you have scrapped cases up to December 30 of that year than you have scraped all of the cases. To be safe put in a higher              upper bound than the last cases for December and if nothing comes back then you will know you got them all. This can be a bit of trial and error because there may              be big gaps in the case numbers for the foreclosure cases we want to scrape. \n","\n","5. Name of output file name (must be csv file path, this will be saved onto your desktop)\n","\n","Save completed file in the appropriate folder for raw data- make sure mortgage foreclosure data is saved with mortgage data and tax with tax. There will be additional files that are saved to your desktop during the scraping process. These can be moved to the trash after the raw file is saved. \n","\n","*NOTE: If you are scraping a lot of data be sure to make each file pathway a unique name, saving multiple scraping sessions as the same csv name will cause errors.\n"]},{"cell_type":"markdown","metadata":{"id":"vUWmBoZZfhOc"},"source":["# Background & Resources"]},{"cell_type":"markdown","metadata":{"id":"RGGDOiJ-YIrp"},"source":["This little section wont actually help you with anything. But it's nice to have."]},{"cell_type":"markdown","metadata":{"id":"1PDCakVVdzPg"},"source":["[MD Access to Judicial Records](https://govt.westlaw.com/mdc/Browse/Home/Maryland/MarylandCodeCourtRules?guid=NDF9067D03C9711E6B2B6BCAD65966614&originationContext=documenttoc&transitionType=Default&contextData=(sc.Default) )\n","\n","mdcourts \n","- [faq](https://mdcourts.gov/casesearch2/faq)\n","- [termsdisclaimer](https://mdcourts.gov/reference/termsdisclaimer)\n","- [estatesearchglossary](https://mdcourts.gov/casesearch2/estatesearchglossary)"]},{"cell_type":"markdown","metadata":{"id":"wMbm7iGFeHf9"},"source":["Other Helpful search tips:\n","\n","http://www.registers.state.md.us. EstateSearch provides public Internet access to information from estate records maintained by the Maryland Registers of Wills. This information includes decedent’s name, estate number and status, date of death, date of filing, personal representative, attorney, decedent alias, and docket history. All records are available from 1998 to present for all estates filed with the Register of Wills’ office. The data is updated daily at the end of the business day.\n","\n","Use a % as a wildcard when searching in a field (Smith%) would give you all names that start with Smith, Smithson, Smithsburg, Smithman, etc.\n","When searching for a date range you need to enter a last name or first name (partials allowed)\n","You can sort the columns by clicking on the column header.\n","Click the Search again option to take you back to your previous search criteria.\n","Use the clear button to clear all fields and begin your search again.\n","\n","List of Estate Types\n","\n","- Regular Estate (RE) - Assets subject to administration in excess of $30,000 ($50,000 if the spouse is the sole legatee or heir).\n","- Regular Estate Judicial (RJ) - A proceeding conducted by the Orphans' Court when matters cannot be handled administratively. For example, when the validity of the will is at issue, or the will is lost, stolen or damaged.\n","- Small Estate (SE) - Assets subject to administration valued at $30,000 or less ($50,000 if the spouse is the sole legatee or heir).\n","-Small Estate Judicial (SJ) - A proceeding conducted by the Orphans' Court when matters cannot be handled administratively. For example, when the validity of the will is at issue, or the will is lost, stolen or damaged.\n","- Foreign Proceeding (FP) - Decedent domiciled out of state with real property in Maryland.\n","- Motor Vehicle (MV) - Transfer of motor vehicle only.\n","- NonProbate (NP) - Property of the decedent which passes by operation of law such as a joint tenancy, tenants by the entireties, or property passing under a deed or trust, revocable or irrevocable. Non probate property must be reported to the Register of Wills on the Information Report or Application to Fix Inheritance Tax on Non-Probate assets.\n","- Unprobated Will Only (UN) - Will and Information Report filed with will and/or Application to Fix Inheritance Tax.\n","- Modified Administration (MA) - A procedure available when the residual legatees consists of the personal representative, spouse; and children. Estate is solvent, Final Distribution can occur within 12 months from date of appointment. A verified final report is filed within 10 months from the date of appointment.\n","- Guardianship Estate (GE) - Guardianship of property for a minor.\n","- Limited Order (LO) – A limited order to locate assets or a will."]},{"cell_type":"markdown","metadata":{"id":"GAn329L8ez8j"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"HJihPyPhqP8x"},"source":["%%capture\n","! pip install mechanicalsoup\n","! pip install urlopen"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZu_L7W2-UAE","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1632163948194,"user_tz":240,"elapsed":257,"user":{"displayName":"Baltimore Neighborhood Indicators Alliance","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-q3A7kTIHf7wESw989OAOaJeUYKXGEdoA4M3NTA=s64","userId":"16379023391965073054"}},"outputId":"9a31c5e6-98e4-4832-b897-057565fae29d"},"source":["#export\n","import mechanicalsoup\n","mechanicalsoup.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.1.0'"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"hCSp69Dou6tF"},"source":["## Scrape"]},{"cell_type":"code","metadata":{"id":"GxkYf7CfbDkk"},"source":["#export\n","description = \"\"\"settings\n","\n","Configuration settings and global variables for the entire project. This bit\n","is intended to only be used as a non-executable script.\n","\"\"\"\n","\n","from os import path\n","\n","# browser settings\n","HEADER = (\"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.1)\"\n","          \" Gecko/2008071615 Fedora/3.0.1-1.fc9 Firefox/3.0.1\")\n","URL = \"http://casesearch.courts.state.md.us/casesearch/\"\n","CASE_PAT = \"24{type}{year}00{num}\"\n","\n","# scraping parameters\n","CASE_TYPES = ('O', 'C')\n","\n","# temporary directory settings\n","HTML_DIR = \"responses\"\n","HTML_FILE = path.join(HTML_DIR, \"{case}\")\n","\n","# log file settings\n","CHECKPOINT = \"checkpoint.json\"\n","NO_CASE = \"no_case.json\"\n","\n","# data mining settings\n","FEATURES = [\n","    \"Filing Date\",\n","    \"Case Number\",\n","    \"Case Type\",\n","    \"Title\",\n","    \"Plaintiff\",\n","    \"Defendant\",\n","    \"Address\",\n","    \"Business or Organization Name\",\n","    \"Party Type\",\n","]\n","FIELDS = FEATURES + [ \"Zip Code\", \"Partial Cost\" ]\n","INTERNAL_FIELDS = [ \"Business or Organization Name\", \"Party Type\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tkrJRvsCwL4E"},"source":["### Local Browser"]},{"cell_type":"code","metadata":{"id":"SFC5Kz6gyEcG"},"source":["#export\n","description = \"\"\"local_browser\n","\n","NOTICE: Close Crawl formerly ran its browser form submissions through Mechanize.\n","The module, however, is deprecated and does not support Python 3. The more\n","stable and maintained Mechanize and BeautifulSoup wrapper, MechanicalSoup,\n","has since replaced the Mechanize methods to support Python 3.\n","\n","This module contains the configurations and settings for the browser used for\n","crawling and scraping through the pages in Close Crawl. The script contains the\n","implementation of the Session class which inherits attributes from the classobj\n","mechanize.Browser()\n","\n","The script worked as an internal module for original Close Crawl executable, and could be imported\n","as a module for testing purposes.\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zXKAJrfLyClo"},"source":["#export\n","from __future__ import absolute_import, print_function, unicode_literals\n","# import cookielib \n","import http.cookiejar as cookielib # for Python3\n","import warnings\n","from urllib.request import urlopen\n","# from urllib import urlopen urllib.request\n","\n","## from mechanize import Browser, _http\n","import mechanicalsoup\n","\n","# from settings import HEADER, URL\n","\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MqAHB55yaB1L"},"source":["Used by our Spider class. This will help our Spider by initiating a browser sesion and progressing us past the websites authorization homepage and filter the court cases for us."]},{"cell_type":"code","metadata":{"id":"ACGn1Rr2wNoB"},"source":["#export\n","class Session(object):\n","\n","    def __init__(self):\n","        \"\"\"Constructor\n","\n","        Args:\n","            None\n","\n","        Attributes:\n","            browser (`mechanize._mechanize.Browser`): browser object in session\n","        \"\"\"\n","\n","        self.browser = mechanicalsoup.StatefulBrowser()\n","\n","        # set error and debug handlers for the browser\n","\n","        # cookie jar\n","        self.browser.set_cookiejar(cookielib.LWPCookieJar())\n","\n","        # browser options\n","        # self.browser.set_handle_equiv(True)\n","        # self.browser.set_handle_gzip(True)\n","        # self.browser.set_handle_redirect(True)\n","        # self.browser.set_handle_referer(True)\n","        # self.browser.set_handle_robots(False)\n","\n","        # follows refresh 0 but doesn't hang on refresh > 0\n","        #self.browser.set_handle_refresh( _http.HTTPRefreshProcessor(), max_time=1 )\n","\n","        # user-Agent\n","        # self.browser.addheaders = [(\"User-agent\", HEADER)]\n","\n","    def close(self):\n","        \"\"\"Destructor for Session. Closes current browser session\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        self.browser.close()\n","\n","    def case_id_form(self, case):\n","        \"\"\"Grabs the form in the case searching page, and inputs the\n","        case number to return the response.\n","\n","        Args:\n","            case (`str`): case ID to be scraped\n","\n","        Returns:\n","            response (`str`): HTML response\n","        \"\"\"\n","\n","        # iterate through the forms to find the correct one\n","        #for form in self.browser.forms():\n","        #    if form.attrs[\"name\"] == \"inquiryFormByCaseNum\":\n","        #        self.browser.form = form\n","        #        break\n","        \n","        self.browser.select_form('form[action=\"/casesearch/inquiryByCaseNum.jis\"]') \n","\n","        # submit case ID and return the response\n","        self.browser[\"caseId\"] = case\n","        response = self.browser.submit_selected()\n","        response = response.text\n","        # if any( case_type in response.upper() for case_type in (\"FORECLOSURE\", \"FORECLOSURE RIGHTS OF REDEMPTION\", \"MOTOR TORT\") ): print (response.upper)\n","\n","        self.browser.open(\"http://casesearch.courts.state.md.us/casesearch/inquiryByCaseNum.jis\")\n","        # , \"MOTOR TORT\"\n","        return response if any(\n","            case_type in response.upper() for case_type in\n","            (\"FORECLOSURE\", \"FORECLOSURE RIGHTS OF REDEMPTION\")\n","        ) else False\n","\n","    def disclaimer_form(self):\n","        \"\"\"Navigates to the URL to proceed to the case searching page\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","\n","        # visit the site\n","        print(URL)\n","        self.browser.open(\"http://casesearch.courts.state.md.us/casesearch/\")\n","\n","        # select the only form on the page\n","        self.browser.select_form('form')\n","\n","        with open(\"output1.html\", \"w\") as file:\n","            file.write(str( self.browser.page ))\n","        chkbxid = self.browser.page.find('input',{'name':'disclaimer'})['value']\n","            \n","        # select the checkbox\n","        self.browser[\"disclaimer\"] = [chkbxid]\n","\n","        # submit the form\n","        self.browser.submit_selected()\n","\n","    @staticmethod\n","    def server_running():\n","        \"\"\"Checks the status of the Casesearch servers\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            `True` if server is up, `False` otherwise\n","        \"\"\"\n","        return urlopen(URL).getcode() == 200\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uchYLV629hp6"},"source":["### Spider"]},{"cell_type":"code","metadata":{"id":"EdHlEWvfEn7R"},"source":["#export\n","from __future__ import absolute_import, print_function, unicode_literals\n","from json import dumps, load\n","from os import path, makedirs\n","from random import uniform\n","import sys\n","from time import sleep\n","\n","from tqdm import trange\n","\n","# from local_browser import Session\n","# from settings import CASE_PAT, CHECKPOINT, HTML_DIR, HTML_FILE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V6r33jRPaukC"},"source":["The spider class will initate the browser session with the parameters you pass it. It scrapes 500 at a time before timing out. This is done so we do not spam their servers."]},{"cell_type":"code","metadata":{"id":"qX1yv91yEp3x"},"source":["#export\n","class Spider(object):\n","\n","    def __init__(self, case_type, year, bounds=range(1, 6), gui=False):\n","\n","        # initial disclaimer page for terms and agreements\n","        self.browser = Session()\n","\n","        if not self.browser.server_running():\n","            sys.exit(\"Server is unavailable at the moment\")\n","\n","        print (self.browser)\n","        \n","        self.browser.disclaimer_form()\n","\n","        self.WAITING_TIME = 0\n","        self.case_type = case_type\n","        self.year = year\n","        self.bounds = bounds\n","\n","        if not path.exists(HTML_DIR):\n","            makedirs(HTML_DIR)\n","\n","    def save_response(self):\n","\n","        case_range = trange(\n","            len(self.bounds), desc=\"Crawling\", leave=True\n","        )\n","\n","        for case_num in case_range:\n","\n","            if case_num and not case_num % 500:\n","                print(\"500 CASES SCRAPED. SCRIPT WILL WAIT 5 MINUTES TO RESUME\")\n","\n","                for i in range(150, 0, -1):\n","                    sleep(1)\n","                    sys.stdout.write('\\r' + \"%02d:%02d\" % divmod(i, 60))\n","                    sys.stdout.flush()\n","\n","            case = CASE_PAT.format(\n","                type=self.case_type,\n","                year=self.year,\n","                num=\"{:04d}\".format(int(str(self.bounds[case_num])[-4:]))\n","            )\n","\n","            try:\n","\n","                wait = uniform(0.0, 0.5)\n","                sleep(wait)\n","\n","                self.WAITING_TIME += wait\n","\n","                case_range.set_description(\"Crawling {}\".format(case))\n","\n","                stripped_html = self.browser.case_id_form(case)\n","                # print('returend this ' , stripped_html)\n","\n","                if stripped_html:\n","                    with open(\n","                        HTML_FILE.format(case=case) + \".html\", 'w'\n","                    ) as case_file:\n","                        case_file.write(str(stripped_html))\n","\n","            # pause process\n","            except KeyboardInterrupt:\n","\n","                self.dump_json({\n","                    \"error_case\":\n","                        \"{:04d}\".format(int(str(self.bounds[case_num])[-4:])),\n","                        \"year\": self.year,\n","                        \"type\": self.type\n","                })\n","                print(\"Crawling paused at\", case)\n","                break\n","\n","            # case does not exist\n","            except IndexError:\n","\n","                self.dump_json({\"error_case\": case})\n","                print(case, \"does not exist\")\n","                break\n","\n","        # close browser and end session\n","        self.close_sesh()\n","\n","    @staticmethod\n","    def dump_json(data):\n","\n","        with open(CHECKPOINT, \"r+\") as checkpoint:\n","            checkpoint_data = load(checkpoint)\n","\n","            for key, val in data.items():\n","                checkpoint_data[key] = val\n","\n","            checkpoint_data[key] = data\n","            checkpoint.seek(0)\n","            checkpoint.write(dumps(checkpoint_data))\n","            checkpoint.truncate()\n","\n","    def close_sesh(self):\n","        self.browser.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gnRJ2RDJLxA"},"source":["This bit will call the spider to do the scraping. We will need to extract the information once obtained. We use some of the set parameters from above here."]},{"cell_type":"code","metadata":{"id":"gh9MXtpeCOFq"},"source":["scrape = True\n","if scrape:\n","  spider = Spider(\n","    case_type=casetype, year=year,\n","    bounds=range(lowerbound, upperbound), gui=False\n","  )\n","  spider.save_response()\n","  wait = spider.WAITING_TIME"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrwQHlZWg4mu"},"source":["# Miner"]},{"cell_type":"markdown","metadata":{"id":"-yoZh4Bqg-TS"},"source":["## Patterns"]},{"cell_type":"code","metadata":{"id":"t6NqQ75FhiGF"},"source":["#export\n","description = \"\"\"Patterns\n","\n","Regular expression patterns and string filtering functions implemented in the\n","project. This file is intended to only be used as a non-executable script.\n","\n","(\\d{1,4}\\s[\\w\\s]{1,20}((?:st(reet)?|ln|lane|ave(nue)?|r(?:oa)?d|highway|hwy|\n","dr(?:ive)?|sq(uare)?|tr(?:ai)l|c(?:our)?t|parkway|pkwy|cir(cle)?|ter(?:race)?|\n","boulevard|blvd|pl(?:ace)?)\\W?(?=\\s|$))(\\s(apt|block|unit)\\W?([A-Z]|\\d+))?)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38X8chjVhjL0"},"source":["#export\n","from re import compile as re_compile\n","from re import I as IGNORECASE\n","from string import punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DmEvCEyufYb7"},"source":["We use this stuff for the miner below when working on the pulled files created from the parts above."]},{"cell_type":"code","metadata":{"id":"AqPf-3a7g9R9"},"source":["#export\n","PUNCTUATION = punctuation.replace('#', '')  # all punctuations except '#'\n","\n","street_address = re_compile(\n","    \"(\"  # begin regex group\n","    \"\\d{1,4}\\s\"  # house number\n","    \"[\\w\\s]{1,20}\"  # street name\n","    \"(\"  # start street type group\n","    \"(?:st(reet)?|ln|lane|ave(nue)?\"  # (st)reet, lane, ln, (ave)nue\n","    \"|r(?:oa)?d|highway|hwy|dr(?:ive)?\"  # rd, road, hwy, highway, (dr)ive\n","    \"|sq(uare)?|tr(?:ai)l|c(?:our)?t\"  # (sq)uare, (tr)ail, ct, court\n","    \"|parkway|pkwy|cir(cle)?|ter(?:race)?\"  # parkway, pkwy, (cir)cle, (ter)race\n","    \"|boulevard|blvd|pl(?:ace)?\"  # boulevard, bvld, (pl)ace\n","    \"\\W?(?=\\s|$))\"  # look ahead for whitespace or end of string\n","    \")\"  # end street type group\n","    \"(\\s(apt|block|unit)(\\W|#)?([\\d|\\D|#-|\\W])+)?\"  # apt, block, unit number\n","    \")\",  # end regex group\n","    IGNORECASE  # case insensitive flag\n",")\n","\n","# case insensitive delimiter for Titles\n","TITLE_SPLIT_PAT = re_compile(\" vs \", IGNORECASE)\n","\n","# pattern for Baltimore zip codes\n","ZIP_STR = \"2\\d{4}\"\n","ZIP_PAT = re_compile(ZIP_STR)\n","\n","# regex pattern to capture monetary values between $0.00 and $999,999,999.99\n","# punctuation insensitive\n","MONEY_STR = \"\\$\\d{,3},?\\d{,3},?\\d{,3}\\.?\\d{2}\"\n","MONEY_PAT = re_compile(MONEY_STR)\n","\n","NULL_ADDR = re_compile(\n","    \"^(\"\n","    \"(\" + MONEY_STR + \")\"\n","    \"|(\" + ZIP_STR + \")\"\n","    \"|(\\d+)\"\n","    \"|(\" + ZIP_STR + \".*\" + MONEY_STR + \")\"\n","    \")$\",\n","    IGNORECASE\n",")\n","\n","STRIP_ADDR = re_compile(\n","    \"(balto|\" + ZIP_STR + \"|md|\" + MONEY_STR + \").*\",\n","    IGNORECASE\n",")\n","\n","\n","def filter_addr(address):\n","\n","    try:\n","      return ''.join(\n","        street_address.search( \n","          address.translate( str.maketrans('','', PUNCTUATION) ) \n","        ).group(0)\n","      )\n","    except AttributeError:\n","      return ''\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jJSoI9gg9vc"},"source":["## Miner"]},{"cell_type":"code","metadata":{"id":"Ki2aj3vGhQNc"},"source":["#export\n","\"\"\"Miner\"\"\"\n","from __future__ import absolute_import, print_function, unicode_literals\n","from csv import DictWriter\n","from json import dump, dumps, load\n","from os import path\n","\n","from bs4 import BeautifulSoup\n","from tqdm import trange\n","\n","# from patterns import MONEY_PAT, TITLE_SPLIT_PAT, ZIP_PAT, filter_addr\n","# from settings import HTML_FILE, NO_CASE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddzyvqjJftnM"},"source":["As stated above. The miner will take the downloaded files and extract the list of features."]},{"cell_type":"code","metadata":{"id":"fkKl_W0Og9ls"},"source":["#export \n","# data mining settings\n","FEATURES = [\n","    \"Filing Date\",\n","    \"Case Number\",\n","    \"Case Type\",\n","    \"Title\",\n","    \"Plaintiff\",\n","    \"Defendant\",\n","    \"Address\",\n","    \"Business or Organization Name\",\n","    \"Party Type\",\n","]\n","FIELDS = FEATURES + [ \"Zip Code\", \"Partial Cost\" ]\n","INTERNAL_FIELDS = [ \"Business or Organization Name\", \"Party Type\"]\n","\n","class Miner(object):\n","\n","    def __init__(self, responses, output, debug=False):\n","\n","        self.responses = responses\n","        self.output = output\n","        self.debug = debug\n","        self.dataset = []\n","        self.maybe_tax = False\n","        self.features = [i + ':' for i in FEATURES]\n","\n","    def scan_files(self):\n","\n","        case_range = trange(len(self.responses), desc=\"Mining\", leave=True) \\\n","            if not self.debug else range(len(self.responses))\n","\n","        for file_name in case_range:\n","            with open(\n","                HTML_FILE.format(case=self.responses[file_name]), 'r'\n","            ) as html_src:\n","\n","                if not self.debug:\n","                    case_range.set_description(\n","                        \"Mining {}\".format(self.responses[file_name])\n","                    )\n","\n","                feature_list = self.scrape(html_src.read())\n","                row = self.distribute(feature_list)\n","\n","                if not row:\n","\n","                    if not path.isfile(NO_CASE):\n","                        with open(NO_CASE, 'w') as no_case_file:\n","                            dump([], no_case_file)\n","\n","                    with open(NO_CASE, \"r+\") as no_case_file:\n","                        no_case_data = load(no_case_file)\n","                        no_case_data.append(str(self.responses[file_name][:-5]))\n","                        no_case_file.seek(0)\n","                        no_case_file.write(dumps(sorted(set(no_case_data))))\n","                        no_case_file.truncate()\n","\n","                self.dataset.extend(row)\n","\n","    def export(self):\n","\n","        file_exists = path.isfile(self.output)\n","\n","        with open(self.output, 'a') as csv_file:\n","            writer = DictWriter(\n","                csv_file,\n","                fieldnames=[\n","                    col for col in FIELDS if col not in INTERNAL_FIELDS\n","                ]\n","            )\n","\n","            if not file_exists:\n","                writer.writeheader()\n","\n","            for row in self.dataset:\n","                writer.writerow(row)\n","\n","    def scrape(self, html_data):\n","        \"\"\"Scrapes the desired features\n","\n","        Args:\n","            html_data: <str>, source HTML\n","\n","        Returns:\n","            scraped_features: <dict>, features scraped and mapped from content\n","        \"\"\"\n","\n","        soup = BeautifulSoup(html_data, \"html.parser\")\n","\n","        # Search for the word 'tax in the document'\n","        if \"tax\" in soup.text.lower():\n","            self.maybe_tax = True\n","\n","        # Create an array from all TR's with the inner HTML for each TR\n","        # Data we want is stored inside an arbitrary # of 'span' tags inside the TR's.\\\n","        tr_list = soup.find_all(\"tr\")\n","\n","        # This will create an array for each TR with an array of SPAN values inside.  \n","        feature_list = []\n","        for tag in tr_list:\n","            try:\n","                # Create an innerhtml array for all spans within a single TR\n","                tag = [j.string for j in tag.findAll(\"span\")]\n","                if set(tuple(tag)) & set(self.features):\n","                    try:\n","                        # Save the spans inner HTML if its not a header label\n","                        tag = [i for i in tag if \"(each\" not in i.lower()]\n","                    except AttributeError:\n","                        continue\n","                    feature_list.append(tag)\n","\n","            except IndexError:\n","                continue\n","\n","        # feature_list is an array [tr] of arrays [spans]. we want this flattened.\n","        # [tr1span1KEY, tr1span1VALUE, tr1span2KEY, tr1span2VALUE, tr2span1KEY, tr2span1VALUE, ]\n","        try:\n","            # flatten multidimensional list\n","            feature_list = [\n","                item.replace(':', '')\n","                for sublist in feature_list for item in sublist\n","            ]\n","\n","        except AttributeError:\n","            pass\n","\n","        return feature_list\n","\n","    def distribute(self, feature_list):\n","\n","        # feature_list ~= [html][tr][spans].innterHTML\n","        # [tr1span1KEY, tr1span1VALUE, tr1span2KEY, tr1span2VALUE, tr2span1KEY, tr2span1VALUE, ]\n","        def __pair(list_type):\n","\n","            # break up elements with n-tuples greater than 2\n","            def __raw_business(i): return any( x in feature_list[i:i + 2][0] for x in INTERNAL_FIELDS )\n","            def __feature_list(i): return feature_list[i:i + 2][0] in FEATURES\n","            condition = __raw_business if list_type else __feature_list\n","\n","            # then convert list of tuples to dict for faster lookup\n","            return [ tuple(feature_list[i:i + 2]) for i in range(0, len(feature_list), 2) if condition(i) ]\n","\n","        raw_business = __pair(1) # [(x1,y1),(x2,y2),(x3,y3)] => INTERNAL_FIELDS\n","        feature_list = dict(__pair(0)) # FEATURES\n","        filtered_business = []\n","\n","        # Party_Type = 'property address' not 'plaintiff' or 'defendant'\n","        # Input exists 'Business or Org Name' and == an Address\n","        for label, value in enumerate(raw_business):\n","            try:\n","                party_type = value[1].upper()\n","                section = raw_business[label + 1][0].upper()\n","                flag1 = party_type == \"PROPERTY ADDRESS\"\n","                flag2 = section == \"BUSINESS OR ORGANIZATION NAME\"\n","                if flag1 and flag2: filtered_business.append(raw_business[label + 1])\n","\n","            except IndexError:\n","                print(\"Party Type issue at Case\", feature_list[\"Case Number\"])\n","\n","        scraped_features = []\n","\n","        for address in filtered_business:\n","\n","            str_address = filter_addr(str(address[-1]))\n","            \n","            temp_features = {\n","                key: value for key, value in feature_list.items()\n","                if key in [\"Title\", \"Case Type\", \"Case Number\", \"Filing Date\"]\n","            }\n","\n","\n","            if temp_features[\"Case Type\"].upper() == \"FORECLOSURE\":\n","                temp_features[\"Case Type\"] = \"Mortgage\"\n","\n","            elif temp_features[\"Case Type\"].upper() == \\\n","                    \"FORECLOSURE RIGHTS OF REDEMPTION\" and self.maybe_tax:\n","                temp_features[\"Case Type\"] = \"Tax\"\n","\n","            else:\n","                # break out of the rest of the loop if case type is neither\n","                continue\n","\n","            if 'Title' not in temp_features: \n","              # print('feature_list');\n","              # print(feature_list);\n","              # print('\\n \\n raw_business');\n","              # print(raw_business);\n","              continue\n","\n","            # break up Title feature into Plaintiff and Defendant\n","            try:\n","                temp_features[\"Plaintiff\"], temp_features[\"Defendant\"] = \\\n","                    TITLE_SPLIT_PAT.split(temp_features[\"Title\"])\n","\n","            except ValueError:\n","                temp_features[\"Plaintiff\"], temp_features[\"Defendant\"] = \\\n","                    (\", \")\n","\n","            temp_features[\"Address\"] = \\\n","                str_address if str_address else address[-1]\n","\n","            temp_features[\"Zip Code\"] = ''.join(ZIP_PAT.findall(address[-1]))\n","\n","            temp_features[\"Partial Cost\"] = ''.join(\n","                MONEY_PAT.findall(address[-1])\n","            )\n","\n","            scraped_features.append(temp_features)\n","            temp_features = {}\n","\n","        return scraped_features\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txJ_CMtuJRcu"},"source":["This bit right here will take each html file from the responses folder and mine them for data"]},{"cell_type":"markdown","metadata":{"id":"m_9MqABMgnYa"},"source":["We grab all the files and put em in a list so that they may be mined"]},{"cell_type":"code","metadata":{"id":"Hiqx7Ue0iIyt"},"source":["#export \n","# from settings import CHECKPOINT, HTML_DIR\n","from os import path, remove, walk\n","\n","temp_output = \"temp_data.csv\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kh44eJkhOcMg"},"source":["file_array = [filenames for (dirpath, dirnames, filenames) in walk(HTML_DIR)][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lh-gG37rjC1E"},"source":["miner = Miner(file_array, temp_output)\n","# miner.output = 'tem_dat.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4s7NRF5CjEI-"},"source":["miner.scan_files()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KW1OhQPvjFKY"},"source":["miner.export()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RBkksmmWhpbY"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UD8yqbFoeBeu"},"source":["## Manual Test"]},{"cell_type":"code","metadata":{"id":"B-p8n98beB2w"},"source":["# This can be useful when debugging.\n","code = \"\"\"\n","soup = ''\n","scrapethisfile = '24O19000982.html'\n","with open( HTML_FILE.format(case=scrapethisfile), 'r' ) as html_src:\n","  souptxt = html_src.read()\n","  soup = BeautifulSoup(souptxt, \"html.parser\")\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qWW5dmr5bFGh"},"source":["# This can be useful when debugging.\n","code = \"\"\"scrapethisfile = '24O19000982.html'\n","with open( HTML_FILE.format(case=scrapethisfile), 'r' ) as html_src:\n","  print( scrapethisfile);\n","  feature_list = scrape(html_src.read())\n","  print(feature_list);\n","  row = distribute(feature_list)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEB_C0FA1vu7"},"source":["# This can be useful when debugging.\n","code = \"\"\"\n","# from settings import CHECKPOINT, HTML_DIR\n","from os import path, remove, walk\n","\n","temp_output = \"temp_dat.csv\"\n","file_array = [filenames for (dirpath, dirnames, filenames) in walk(HTML_DIR)][0]\n","top5 = [ file_array[41] ]\n","miner = Miner(top5, temp_output)\n","miner.scan_files()\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K1OUa3XztaOg"},"source":["# Cleaner"]},{"cell_type":"code","metadata":{"id":"tMkruUBVtEmu"},"source":["#export \n","description = \"\"\"Cleaner\n","\n","This module implements post-scraping cleaning processes on the raw initial\n","dataset. Processes include stripping excess strings off Address values,\n","removing Zip Code and Partial Cost values mislabeled as Address, and merging\n","rows containing blank values in alternating features.\n","\n","The script works as an internal module for Close Crawl, but can be executed\n","as a standalone to manually process datasets:\n","\n","    $ python cleaner.py <path/to/old/dataset> <path/of/new/dataset>\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJhRzfR6tdKe"},"source":["#export \n","from __future__ import absolute_import, print_function, unicode_literals\n","from pandas import DataFrame, concat, read_csv, to_datetime\n","# from patterns import NULL_ADDR, STRIP_ADDR, filter_addr, punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7TtTB-UtiPw"},"source":["#export \n","class Cleaner(object):\n","    \"\"\"Class object for cleaning the raw dataset extracted after the initial\n","    scraping\n","    \"\"\"\n","\n","    def __init__(self, path):\n","        \"\"\"Constructor for Cleaner\n","\n","        Args:\n","            path (`str`): path to input CSV dataset\n","\n","        Attributes:\n","            df (`pandas.core.frame.DataFrame`): initial DataFrame\n","            columns (`list` of `str`): columns of the DataFrame\n","            clean_df (`pandas.core.frame.DataFrame`): final DataFrame to be\n","                outputted\n","        \"\"\"\n","\n","        self.df = self.prettify(read_csv(path))\n","\n","        self.columns = list(self.df)\n","        self.clean_df = []\n","\n","    @staticmethod\n","    def prettify(df, internal=True):\n","        \"\"\"Drops duplicates, sorts and fills missing values in the DataFrame\n","        to make it manageable.\n","\n","        Args:\n","            df (`pandas.core.frame.DataFrame`): DataFrame to be managed\n","            internal (`bool`, optional): flag for determining state of\n","                DataFrame\n","\n","        Returns:\n","            df (`pandas.core.frame.DataFrame`): organized DataFrame\n","        \"\"\"\n","\n","        df.drop_duplicates(inplace=True, keep='last', subset=[\"Case Number\"] )\n","        df[\"Filing Date\"] = to_datetime(df[\"Filing Date\"])\n","\n","        df.sort_values(\n","            [\"Filing Date\", \"Case Number\", \"Address\"],\n","            ascending=[True] * 3,\n","            inplace=True\n","        )\n","\n","        if internal:\n","            df[\"Zip Code\"] = df[\"Zip Code\"].fillna(0.0).astype(int)\n","            df[\"Zip Code\"] = df[\"Zip Code\"].replace(0, '')\n","        return df\n","\n","    def clean_addr(self):\n","        \"\"\"Cleans excess strings off Address values and removes Zip Code and\n","        Partial Cost values mislabeled as Address.\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","\n","        def clean_string(addr):\n","            \"\"\"Applies regular expressions and other filters on Address\n","            values\n","\n","            Args:\n","                addr (`str`): Address value to be filtered\n","\n","            Returns:\n","                addr (`str`): filtered Address value\n","            \"\"\"\n","\n","            # if value does not match the street_address pattern\n","            if not filter_addr(addr):  # patterns.filter_addr\n","\n","                if NULL_ADDR.sub('', addr):  # value may contain valid Address\n","                    return str(\n","                        STRIP_ADDR.sub(\n","                            '', addr)  # strip off Zip Code and Partial Cost\n","                    ).translate(\n","                        {ord(c): None for c in punctuation}\n","                    ).strip()  # strip off punctuations\n","\n","            return addr\n","\n","        print(\"Cleaning addresses...\", end=\" \")\n","\n","        self.df[\"Address\"] = self.df[\"Address\"].apply(\n","            lambda x: clean_string(x)\n","        )\n","        self.df[\"Address\"] = self.df[\"Address\"].apply(\n","            lambda x: NULL_ADDR.sub('', x)\n","        )\n","\n","        # replace empty string values with NULL\n","        self.df[\"Zip Code\"] = self.df[\"Zip Code\"].replace('', float(\"nan\"))\n","        self.df[\"Address\"] = self.df[\"Address\"].replace('', float(\"nan\"))\n","\n","        print(\"Done\")\n","\n","    @staticmethod\n","    def combine_rows(row):\n","        \"\"\"Merges rows after filtering out common values\n","\n","        Args:\n","            row (`list` of `list` of `str`): groupby(\"Case Number\") rows\n","\n","        Returns:\n","            (`list` of `str`): merged row\n","        \"\"\"\n","\n","        def __filter_tuple(col):\n","            \"\"\"Filters common values from rows\n","\n","            Args:\n","                col (`tuple` of `str`): values per column\n","\n","            Returns:\n","                value (`str`): common value found per mergeable rows\n","            \"\"\"\n","\n","            for value in set(col):\n","                if value == value:  # equivalent to value != NaN\n","                    return value\n","\n","        return [__filter_tuple(x) for x in zip(*row)]\n","\n","    @staticmethod\n","    def mergeable(bool_vec):\n","        \"\"\"Determines if groupby(\"Case Number\") rows are mergeable\n","\n","        Example:\n","            bool_vec = [\n","                [True, True, True, True, True, True, False, True, True],\n","                [True, True, True, True, True, True, True, False, False],\n","                [True, True, True, True, True, True, False, False, False]\n","            ]\n","\n","            __sum_col(bool_vec) -> [3, 3, 3, 3, 3, 3, 1, 1, 1]\n","\n","            __bool_pat(__sum_col(bool_vec)) -> True\n","\n","        Args:\n","            bool_vec (`list` of `bool`): represents non-NULL values\n","\n","        Returns:\n","            (`bool`): True if rows are mergeable\n","        \"\"\"\n","\n","        def __sum_col():\n","            \"\"\"Sums columns\n","\n","            Args:\n","                None\n","\n","            Returns:\n","                (`list` of `int`): sum of columns\n","            \"\"\"\n","            return [sum(x) for x in zip(*bool_vec)]\n","\n","        def __bool_pat(row):\n","            \"\"\"Determines mergeability\n","\n","            Args:\n","                None\n","\n","            Returns:\n","                (`bool`): True if rows are mergeable\n","            \"\"\"\n","            return set(row[-3:]) == set([1]) and set(row[:-3]) != set([1])\n","\n","        return True if __bool_pat(__sum_col()) else False\n","\n","    def merge_nulls(self):\n","        \"\"\"Splits DataFrames into those with NULL values to be merged, and then\n","        later merged with the original DataFrame\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","\n","        print(\"Merging rows...\", end=\" \")\n","        print(self.df)\n","\n","        # filter out rows with any NULL values\n","        origin_df = self.df.dropna()\n","\n","        # filter out rows only with NULL values\n","        null_df = self.df[self.df.isnull().any(axis=1)]\n","\n","        # boolean representation of the DataFrame with NULL values\n","        bool_df = null_df.notnull()\n","\n","        # (`list` of `dict` of `str` : `str`) to be converted to a DataFrame\n","        new_df = []\n","\n","        for i in null_df[\"Case Number\"].unique():\n","            bool_row = bool_df[null_df[\"Case Number\"] == i]\n","            new_row = null_df[null_df[\"Case Number\"] == i]\n","\n","            # if the rows are mergeable, combine them\n","            if self.mergeable(bool_row.values):\n","                new_row = self.combine_rows(new_row.values.tolist())\n","\n","                new_df.append(\n","                    {\n","                        feature: value\n","                        for feature, value in zip(self.columns, new_row)\n","                    }\n","                )\n","\n","            # else, treat them individually\n","            else:\n","                new_row = new_row.values.tolist()\n","\n","                for row in new_row:\n","                    new_df.append(\n","                        {\n","                            feature: value\n","                            for feature, value in zip(self.columns, row)\n","                        }\n","                    )\n","\n","        # merge the DataFrames back\n","        self.clean_df = concat(\n","            [origin_df, DataFrame(new_df)]\n","        ).reset_index(drop=True)\n","\n","        # prettify the new DataFrame\n","        self.clean_df = self.prettify(\n","            self.clean_df[self.columns], internal=False\n","        )\n","\n","        print(\"Done\")\n","\n","    def init_clean(self):\n","        \"\"\"Initializes cleaning process\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        print(self.df)\n","        self.clean_addr()\n","        print(self.df)\n","        self.merge_nulls()\n","\n","    def download(self, output_name):\n","        \"\"\"Downloads the cleaned and manipulated DataFrame into a CSV file\n","\n","        Args:\n","            output_name (`str`): path of the new output file\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        self.clean_df.rename(columns={\"Address\": \"address\"}, inplace=True)\n","        self.clean_df.to_csv(output_name, index=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3vL8wy5lhDN"},"source":["temp_output = \"temp_data.csv\"\n","df_obj = Cleaner(temp_output)\n","df_obj.init_clean()\n","df_obj.download(output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_qAlZURnaVL"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"DkIgIQk3ne50"},"source":["#export \n","description = \"\"\"main\n","\n","The main executable script for Close Crawl. This file manages types, flags\n","and constraints for the case type, year and output data file.\n","\n","Usage:\n","    $ python main.py <case_type> <case_year> <path/of/new/dataset>\n","      <opt: lower_bound> <opt: upper_bound> <opt: debug>\n","\n","Example usage:\n","    $ python main.py O 2015 test_set.csv -l=300 -u=600 -d=1\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUsaGkBIncGp"},"source":["#export \n","from __future__ import absolute_import, print_function, unicode_literals\n","from json import dump, dumps, load\n","from os import path, remove, walk\n","from shutil import rmtree\n","from time import time "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4brxrwPnaBxq"},"source":["#export \n","def close_crawl(case_type, case_year, output, cases='', lower_bound=0,\n","                upper_bound=0, debug=False, scrape=True, mine=True,\n","                clean=True):\n","    \"\"\"Main function for Close Crawl.\n","\n","    Args:\n","        case_type (`str`): type of foreclosure case, options are 'O' and 'C'\n","        case_year (`str`): year of foreclosure cases\n","        output (`str`): path of the output CSV file, along with the valid\n","            extension (.csv)\n","        lower_bound (`int`, optional): lower bound of range of cases\n","        upper_bound (`int`, optional): upper bound of range of cases\n","        debug (`bool`, optional): option for switching between debug mode.\n","            Default -> True\n","\n","    Returns:\n","        None\n","    \"\"\"\n","\n","    temp_output = \"temp_data.csv\"\n","    wait = 0\n","    case_list = []\n","    \n","    print('checkpoint')\n","    if not path.isfile(CHECKPOINT):\n","        print(\"Initializing project...\")\n","        with open(CHECKPOINT, \"w\") as checkpoint:\n","            dump(\n","                {\n","                    \"last_case\": \"{:04d}\".format(int(str(lower_bound)[-4:])),\n","                    \"type\": case_type,\n","                    \"year\": case_year[-2:],\n","                    \"error_case\": '',\n","                },\n","                checkpoint\n","            )\n","\n","    print('cases')\n","    if not cases:\n","\n","        with open(CHECKPOINT) as checkpoint:\n","            prev_bound = int(load(checkpoint)[\"last_case\"])\n","            if not lower_bound:\n","                lower_bound = prev_bound\n","            upper_bound = upper_bound if int(upper_bound) > int(lower_bound) \\\n","                else str(lower_bound + 5)\n","\n","        case_list = range(int(lower_bound), int(upper_bound) + 1)\n","\n","    else:\n","\n","        with open(cases) as manual_cases:\n","            case_list = sorted(list(set(load(manual_cases))))\n","\n","    print('scrape')\n","    if scrape:\n","        spider = Spider(\n","            case_type=case_type, year=case_year[-2:],\n","            bounds=case_list, gui=False\n","        )\n","\n","        spider.save_response()\n","\n","        wait = spider.WAITING_TIME\n","\n","    print('HTML_DIR', HTML_DIR) \n","    file_array = [filenames for (dirpath, dirnames, filenames)\n","                  in walk(HTML_DIR)][0]\n","\n","    start_mine = time()\n","    print('mine')\n","    if mine:\n","        miner = Miner(file_array, temp_output)\n","        miner.scan_files()\n","        miner.export()\n","\n","\n","    print('clean')\n","    if clean:\n","        df_obj = Cleaner(temp_output)\n","\n","        df_obj.init_clean()\n","        df_obj.download(output)\n","\n","    print('save')\n","\n","    with open(CHECKPOINT, \"r+\") as checkpoint:\n","        checkpoint_data = load(checkpoint) \n","        print(\"checkpoint_data\")\n","        print(checkpoint_data)\n","        checkpoint_data[\"last_case\"] = sorted(file_array)[-1].split('.')[0][-4:]\n","        checkpoint.seek(0)\n","        checkpoint.write(dumps(checkpoint_data))\n","        checkpoint.truncate()\n","\n","    \"\"\"\n","    with open(CHECKPOINT, \"r+\") as checkpoint:\n","        checkpoint_data = load(checkpoint)\n","\n","        for key, val in data.items():\n","            checkpoint_data[key] = val\n","\n","        checkpoint_data[key] = data\n","        checkpoint.seek(0)\n","        checkpoint.write(dumps(checkpoint_data))\n","        checkpoint.truncate()\n","    \"\"\"\n","\n","    # print(\"Crawling runtime: {0:.2f} s\".format((end_crawl - start_crawl)))\n","    # print(\"Downloading runtime: {0:.2f} s\".format( ((end_crawl - start_crawl) - wait)) )\n","    # print(\"Mining runtime: {0:.2f} s\".format((end_mine - start_mine)))\n","    # print(\"Program runtime: {0:.2f} s\".format((end - start)))\n","    print(\"------------ SCRAPING COMPLETED ------------\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PW8EqgMMONjt"},"source":["This example will run everything all at once!"]},{"cell_type":"code","metadata":{"id":"aJYL13g1_icS"},"source":["args = {'type': 'C', 'year': '2020', 'output': 'outputfile.csv', 'file': '', 'lower': '1000', 'upper': '1010', 'debug': '0', 'scrape': True, 'mine': True, 'clean': True}\n","close_crawl(\n","    case_type=args[\"type\"], case_year=args[\"year\"], output=args[\"output\"],\n","    cases=args[\"file\"], lower_bound=args[\"lower\"],\n","    upper_bound=args[\"upper\"], debug=args[\"debug\"],\n","    scrape=args[\"scrape\"], mine=args[\"mine\"],\n","    clean=args[\"clean\"]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1SXQJI6JlNRB"},"source":["## CLI -> Depricated. Do not touch."]},{"cell_type":"code","metadata":{"id":"tAB64vlFoVX5"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPB7zlGzjydH"},"source":["cd ../"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8_w3weqZyxL"},"source":["! python cliargs.py -l=50 -u=3500 -d -s -m C 2016 output.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPN30DKPzv0X"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHghURgeRmv0"},"source":["\"\"\"cliargs\n","\n","The main command line script for Close Crawl. This file manages types, flags\n","and constraints for the case type, year and output data file as well as the\n","processing options.\n","\n","Parameters:\n","  {O,C}        | Type of foreclosure cases\n","  year         | Year of foreclosure cases\n","  output       | Path of output file\n","\n","Optional parameters:\n","  -h, --help   | Show this help message and exit\n","  -v, --version| Show program's version number and exit\n","  -l, --lower  | Lower bound of range of cases\n","  -u, --upper  | Upper bound of range of cases\n","  -f, --file   | Path of JSON array of cases\n","  -d, --debug  | Debug mode\n","  -s, --scrape | Scrape only\n","  -m, --mine   | Mine only\n","  -c, --clean  | Clean only\n","\n","Usage:\n","    $ python cliarg.py [-h] [-v] [-l] [-u] [-f] [-d] [-s] [-m] [-c]\n","                  {O,C} year output\n","\n","Example usages:\n","    $ python cliarg.py -l=50 -u=3500 -d -s -m C 2016 output.csv\n","    $ python cliarg.py -c=\"cases_to_scrape.json\" -d O 2014 output01.csv\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqM4kYpkofWY"},"source":["#export \n","from __future__ import absolute_import, print_function, unicode_literals\n","import sys\n","from textwrap import dedent"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RADe3ugVnM7i"},"source":["#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","\n","if __name__ == \"__main__\":\n","\n","    args = {}\n","\n","    args[\"type\"] = input(\"Enter type of case (1 char: {C, O}): \")\n","    args[\"year\"] = input(\"Enter year of case (4 digit int): \")\n","    args[\"output\"] = input(\"Enter name of output file (CSV file path): \")\n","\n","    opt = int(input(\"Enter 0 for manual parameters or 1 for automatic: \"))\n","    if bool(opt):\n","        args[\"file\"] = input(\"Enter name of cases file (JSON file path): \")\n","        args[\"lower\"] = args[\"upper\"] = 0\n","\n","    else:\n","        args[\"file\"] = \"\"\n","        args[\"lower\"] = input(\"Enter lower bound of cases (1-4 digit int): \")\n","        args[\"upper\"] = input(\"Enter upper bound of cases (1-4 digit int): \")\n","\n","    args[\"debug\"] = input(\n","        \"Enter 0 for default mode, 1 for debug (1 digit int): \"\n","    )\n","\n","    print(\n","        dedent(\n","            \"\"\"Processing options:\\n\\n\"\"\"\n","            \"\"\"For the following options, enter 0 to disable or 1 to enable.\"\"\"\n","            \"\"\"\\nNOTE: The script will exit if all but the mining step is \"\"\"\n","            \"\"\"enabled - data cannot be cleaned without being mined first.\"\"\"\n","        )\n","    )\n","\n","    args[\"scrape\"] = bool(input(\"Scrape: {0, 1}: \"))\n","    args[\"mine\"] = bool(input(\"Mine: {0, 1}: \"))\n","    args[\"clean\"] = bool(input(\"Clean: {0, 1}: \"))\n","\n","    # exit script if all but the mining step is enabled\n","    if (args[\"scrape\"] and not(args[\"mine\"]) and args[\"clean\"]):\n","        sys.exit(\"\\nData cannot be cleaned without being mined first.\")\n","\n","    print (args)\n","    main.close_crawl(\n","        case_type=args[\"type\"], case_year=args[\"year\"], output=args[\"output\"],\n","        cases=args[\"file\"], lower_bound=args[\"lower\"],\n","        upper_bound=args[\"upper\"], debug=args[\"debug\"],\n","        scrape=args[\"scrape\"], mine=args[\"mine\"],\n","        clean=args[\"clean\"]\n","    )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOVybMiTZw_x"},"source":["### Test"]},{"cell_type":"code","metadata":{"id":"a5-boZ9ImREF"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"itQzVZMnZw15"},"source":["! python cliargs.py -h"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6SSGTtuZxoP"},"source":["! python cliargs.py -d -m -c C 2019 output.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0T4ZUTo5gQkg"},"source":["! python cliargs.py -l=9500 -u=9999 -c -s -d -m O 2019 output.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULclUPijn-qT"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXlcPic1uX77"},"source":["if both packages are in your import path (sys.path), and the module/class you want is in example/example.py, then to access the class without relative import try: from example.example import fkt\n","\n","if __name__ == '__main__':\n","    from mymodule import as_int\n","else:\n","    from .mymodule import as_int"]},{"cell_type":"markdown","metadata":{"id":"L09Z9X79zHn6"},"source":["https://stackoverflow.com/questions/448271/what-is-init-py-for\n","\n","Files named __init__.py are used to mark directories on disk as Python package directories. If you have the files\n","\n","mydir/spam/__init__.py\n","mydir/spam/module.py\n","and mydir is on your path, you can import the code in module.py as\n","\n","import spam.module\n","or\n","\n","from spam import module"]}]}