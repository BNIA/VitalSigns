{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"91_VitalSigns_Acs_Download.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"R44Hy0QAmDqX"},"source":["# ACS Download\n","\n","## ACS TOOL STEP 1 -> SETUP : \n","\n","#### Uses: csa2tractcrosswalk.csv, VitalSignsCensus_ACS_Tables.xlsx\n","#### Creates: ./AcsDataRaw/   ./AcsDataClean/"]},{"cell_type":"code","metadata":{"id":"-7vdf5oV7C-9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNFvJ78n7w2T"},"source":["cd 'drive/My Drive/vitalSigns/vs_acs'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gioMjuTLFyw-"},"source":["ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q45_uKIKmDqZ"},"source":["### Import Modules & Construct Path Handlers"]},{"cell_type":"code","metadata":{"id":"CQtiumBjmDqa"},"source":["import os\n","import sys\n","\n","import pandas as pd\n","pd.set_option('display.expand_frame_repr', False)\n","pd.set_option('display.precision', 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUXDP3q5mDqf"},"source":["# Find Relative Path to Files\n","def findFile(root, file):\n","    for d, subD, f in os.walk(root):\n","        if file in f:\n","            return \"{1}/{0}\".format(file, d)\n","            break \n","\n","# To 'import' a script you wrote, map its filepath into the sys\n","def addPath(root, file): sys.path.append(os.path.abspath( findFile( './', file) ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IEUOs_6kmDqj"},"source":["### Get Vital Signs Reference Table"]},{"cell_type":"code","metadata":{"id":"WQ4ZTe9zmDqk"},"source":["file = 'VitalSignsCensus_ACS_Tables.xlsx'\n","xls = pd.ExcelFile(findFile('../', file))\n","acs_tables = pd.read_excel(xls, 'acs_tables')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eaZFLflw8dgc"},"source":["acs_tables.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_9njOm7amDqn"},"source":["### Get Tract/ CSA CrossWalk"]},{"cell_type":"code","metadata":{"id":"XATrW6CVmDqo"},"source":["file = 'csa2tractcrosswalk.csv'\n","crosswalk = pd.read_csv( findFile( '../', file) )\n","crosswalk = dict(zip(crosswalk['TRACTCE10'], crosswalk['CSA2010']  ) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oy99pjoDmDqp"},"source":["### Get retrieve_acs_data function"]},{"cell_type":"code","metadata":{"id":"KBFZAVk1mDqq"},"source":["file = 'retrieve_acs_data.py'\n","addPath( '../../', file)\n","from retrieve_acs_data import retrieve_acs_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kP4gP6XU8NmL"},"source":["#File: retrieveAcsData.py\n","#Author: Charles Karpati\n","#Date: 1/9/19\n","#Section: Bnia\n","#Email: karpati1@umbc.edu\n","#Description:\n","#This file returns ACS data given an ID\n","\n","#def main():\n","#purpose: Retrieves ACS data from the web\n","#input: ID\n","#output: Acs Data. Prints to ../../data/2_cleaned/acs/\n","\n","import pandas as pd\n","import csv\n","from urllib.parse import urlencode\n","\n","# This prevents timeouts\n","import socket\n","socket.setdefaulttimeout(10.0)\n","\n","def retrieve_acs_data(year, tableId):\n","    keys = []\n","    vals = []\n","    header = []\n","    getTheseKeys = ''\n","    getTheseKeys2 = ''\n","    getTheseKeys3 = ''\n","    getTheseKeys4 = ''\n","    keyCount = 0\n","    #~~~~~~~~~~~~~~~\n","    # Step 1)\n","    # Retrieve a Meta Data Table Describing the Content of the Table\n","    #~~~~~~~~~~~~~~~\n","    url = 'https://api.census.gov/data/20'+year+'/acs/acs5/groups/'+tableId+'.json'\n","    print(url);\n","    metaDataTable = pd.read_json(url, orient='records')\n","    \n","    #~~~~~~~~~~~~~~~\n","    # Step 2)\n","    # Createa a Dictionary using the Meta Data Table\n","    #~~~~~~~~~~~~~~~\n","    # Multiple Queries may be Required.\n","    # Max columns returned from any given query is 50.\n","    # For that reasons bin the Columns into Groups of 50.\n","    for key in metaDataTable['variables'].keys():\n","      if key[-1:] == 'E':\n","        keyCount = keyCount + 1\n","        if keyCount < 50 : getTheseKeys = getTheseKeys+','+key\n","        elif keyCount < 99 : getTheseKeys2 = getTheseKeys2+','+key\n","        elif keyCount < 148 : getTheseKeys3 = getTheseKeys3+','+key\n","        else: getTheseKeys4 = getTheseKeys4+','+key\n","        keys.append(key)\n","        val = metaDataTable['variables'][key]['label']\n","        val = key+'_'+val.replace('Estimate!!', '').replace('!!', '_').replace(' ', '_')\n","        vals.append(val)\n","    dictionary = dict(zip(keys, vals))\n","    \n","    #~~~~~~~~~~~~~~~\n","    # Step 3)\n","    # Get the actual data we want with all the columns (obtained using the meta data table)\n","    #~~~~~~~~~~~~~~~\n","    # https://api.census.gov/data/2016/acs/acs5?get=NAME,B11001_002E&for=county:005&in=state:24\n","    urlRoot = 'https://api.census.gov/data/20'+year+'/acs/acs5?'\n","    \n","    def getParams(keys): return {\n","        'get': 'NAME'+keys,\n","        'for': 'tract:*',\n","        'in': 'state:24 county:510',\n","        'key': '829bf6f2e037372acbba32ba5731647c5127fdb0'\n","      }\n","    \n","    def getBCityParams(keys): return {\n","        'get': 'NAME'+keys,\n","        'for': 'county:510',\n","        'in': 'state:24',\n","        'key': '829bf6f2e037372acbba32ba5731647c5127fdb0'\n","      }\n","    \n","    def readIn( url ):\n","        tbl = pd.read_json(url, orient='records')\n","        tbl.columns = tbl.iloc[0]\n","        return tbl \n","    \n","    def appendColumns( table, params):\n","        # Get Tract and City Records For Specific Columns\n","        table2 = readIn( urlRoot+urlencode(getParams(params)) )\n","        table3 = readIn( urlRoot+urlencode(getBCityParams(params)) )\n","        table3['tract'] = '010000'\n","        # Concatenate the Records\n","        table2.append([table2, table3], sort=False)\n","        table2 = pd.concat([table2, table3], ignore_index=True)\n","        # Merge to Master Table\n","        table = pd.merge(table, table2,  how='left', left_on=[\"NAME\",\"state\",\"county\",\"tract\"], right_on = [\"NAME\",\"state\",\"county\",\"tract\"])\n","        return table\n","    \n","    # Get Tract Data\n","    url = urlRoot+urlencode(getParams(getTheseKeys))\n","    table = readIn(url)\n","    table = table.iloc[1:]\n","    \n","    # Get Baltimore City's Data .\n","    url = urlRoot+urlencode(getBCityParams(getTheseKeys))\n","    table2 = readIn(url)\n","    table2 = table2[1:]\n","    table2['tract'] = '010000'\n","    \n","    #Append Baltimore to Tracts\n","    #table = pd.concat([table, table2], keys=[\"NAME\",\"state\",\"county\",], axis=0)\n","    table.append([table, table2], sort=False)\n","    table = pd.concat([table, table2], ignore_index=True)\n","        \n","    if getTheseKeys2 != '' : \n","        table = appendColumns(table, getTheseKeys2)\n","    if getTheseKeys3 != '' : \n","        table = appendColumns( table, getTheseKeys3 )\n","    if getTheseKeys4 != '' : \n","        table = appendColumns( table, getTheseKeys4 )\n","    \n","    \n","    #~~~~~~~~~~~~~~~\n","    # Step 4)\n","    # Prepare Column Names using the meta data table. The raw data has columnsNames in the first row, as well.\n","    # Replace column ID's with labels from the dictionary where applicable (should be always)\n","    #~~~~~~~~~~~~~~~\n","    for column in table.columns:\n","        if column in keys: header.append(dictionary[column])\n","        else: header.append(column)\n","    header = [sub.replace(':', '') for sub in header] \n","    print('HEADERS: ', header)\n","    table.columns = header\n","    #table.drop(table.index[0], inplace=True)\n","    \n","    #~~~~~~~~~~~~~~~\n","    # Step 5) Everything Else\n","    #~~~~~~~~~~~~~~~\n","\n","    # Prettify Names\n","    table['NAME'] = table['NAME'].str.replace(', Baltimore city, Maryland', '')\n","    table['NAME'][table['NAME'] == 'Baltimore city, Maryland'] = 'Baltimore City' \n","    \n","    # Convert to Integers Columns from Strings where Applicable\n","    table = table.apply(pd.to_numeric, errors='ignore')\n","    \n","    return table"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usxjDLQlmDqs"},"source":["### Column Operations"]},{"cell_type":"code","metadata":{"id":"F33B2Z2mmDqt"},"source":["import csv # 'quote all'\n","def fixColNamesForCSV(x): return str(x)[:] if str(x) in [\"NAME\",\"state\",\"county\",\"tract\", \"CSA\"] else str(x)[12:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1senqgvLmDqv"},"source":["## ACS TOOL STEP 2 -> Execute :"]},{"cell_type":"markdown","metadata":{"id":"DLVeo8idmDqw"},"source":["### Save the ACS Data"]},{"cell_type":"code","metadata":{"id":"lBUE2R4qmDqy","scrolled":false},"source":["# Set Index      df.set_index(\"NAME\", inplace = True) \n","# Save raw to    '../../data/3_outputs/acs/raw/'+year+'/'+tableId+'_'+description+'_5y'+year+'_est.csv'\n","# Tract to CSA   df['CSA'] = df.apply(lambda row: crosswalk.get(int(row['tract']), \"empty\"), axis=1)\n","# Save 4 use     '../../data/2_cleaned/acs/'+tableId+'_'+description+'_5y'+year+'_est.csv'\n","\n","year = '19'\n","count = 0\n","startFrom = 0\n","\n","# For each ACS Table\n","for x, row in acs_tables.iterrows():\n","    count += 1\n","\n","    # Grab its Meta Data\n","    description = str(acs_tables.loc[x, 'shortname'])\n","    tableId = str(acs_tables.loc[x, 'id'])\n","    yearExists = int(acs_tables.loc[x, year+'_exists'])\n","\n","    # If the Indicator is valid for the year \n","    # use startFrom to being at a specific count\n","    if yearExists and count >= startFrom:\n","        print(str(count)+') '+tableId + ' ' + description)\n","\n","        # retrieve the Python ACS indicator\n","        print('sending retrieve_acs_data', year, tableId)\n","        df = retrieve_acs_data(year, tableId)\n","\n","        df.set_index(\"NAME\", inplace = True) \n","\n","        # Save the Data as Raw\n","        # We do not want the id in the column names\n","        saveThis = df.rename( columns = lambda x : ( fixColNamesForCSV(x) ) )\n","        saveThis.to_csv('./AcsDataRaw/'+tableId+'_'+description+'_5y'+year+'_est.csv', quoting=csv.QUOTE_ALL)\n","\n","        # Match Tract to CSA\n","        df['CSA'] = df.apply(lambda row: crosswalk.get(int(row['tract']), \"empty\"), axis=1)\n","\n","        # Save the data (again) as Cleaned for me to use in the next scripts\n","        df.to_csv('./AcsDataClean/'+tableId+'_5y'+year+'_est.csv', quoting=csv.QUOTE_ALL)  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUpuxkzM6Lto"},"source":[""],"execution_count":null,"outputs":[]}]}