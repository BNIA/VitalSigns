{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RBIntel.ipynb","provenance":[],"collapsed_sections":["Bq3shPVnj0n0"],"authorship_tag":"ABX9TyOJt0e/vRKJLHJKeNAtoSUn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eHMol4G2gdYW"},"source":["#default_exp rbintel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_jLE_5-I9HzS"},"source":["# Housing -> RBIntel -> Data Intake and Operations\n","\n","> This notebook uses data to generate a portion of BNIA's Vital Signs report."]},{"cell_type":"markdown","metadata":{"id":"iSNOAJoTlQr0"},"source":["This colab and more can be found at https://github.com/BNIA/vitalsigns.\n"]},{"cell_type":"markdown","metadata":{"id":"ukxt0JZCsaxc"},"source":["## Whats Inside?: "]},{"cell_type":"markdown","metadata":{"id":"Jo5GYquJovv-"},"source":["#### __Indicators Used__\n","\n","- ✅ 30 - __dom__ - (RBIntel) Median Number of Days on the Market\n","- ✅ 38 - __cashsa__ - (RBIntel) Percentage of residential sales for cash\n","- ✅ 39 - __reosa__ - (RBIntel) percentage of residential sales in foreclosure (REO)"]},{"cell_type":"markdown","metadata":{"id":"JNveDD1Ho0mY"},"source":["#### __Datasets Used__\n","\n","- ✔️ housing.rbintelregion_201X __(30-dom, 38-cashsa, 39-reosa -> D̶a̶y̶s̶O̶n̶M̶a̶r̶k, BuyerFinan newtrust1l, foreclosur)__\n","\n"]},{"cell_type":"code","metadata":{"id":"r1iNJPWTsNjV"},"source":["year = '18'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8-mgsByhrxG"},"source":["## SETUP Enviornment:"]},{"cell_type":"markdown","metadata":{"id":"0hHCW-qPMeH6"},"source":["### Import Modules"]},{"cell_type":"code","metadata":{"id":"WUvcamATFo4G"},"source":["%%capture\n","! pip install -U -q PyDrive\n","! pip install geopy\n","! pip install geopandas\n","! pip install geoplot\n","! pip install dataplay\n","! pip install matplotlib\n","! pip install psycopg2-binary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0vkoXtZRdJz"},"source":["%%capture\n","! apt-get install build-dep python-psycopg2\n","! apt-get install libpq-dev\n","! apt-get install libspatialindex-dev"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gZ17xjOgR8vg"},"source":["%%capture\n","!pip install rtree\n","!pip install dexplot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9Mcvm-X0gdo"},"source":["from dataplay.geoms import workWithGeometryData"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNOByHFKFo4m"},"source":["%%capture \n","# These imports will handle everything\n","import os\n","import sys\n","import csv \n","import psycopg2\n","import pyproj\n","from pyproj import Proj, transform\n","# conda install -c conda-forge proj4\n","from shapely.geometry import Point\n","from shapely import wkb\n","from shapely.wkt import loads\n","# https://pypi.org/project/geopy/\n","from geopy.geocoders import Nominatim\n","\n","# In case file is KML, enable support\n","import fiona\n","fiona.drvsupport.supported_drivers['kml'] = 'rw'\n","fiona.drvsupport.supported_drivers['KML'] = 'rw'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"evj9GJLdSlxF"},"source":["from IPython.display import clear_output\n","clear_output(wait=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uTcb3bD84mSA"},"source":["import ipywidgets as widgets\n","from ipywidgets import interact, interact_manual"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j1wM01asxvdk"},"source":["#export\n","import numpy as np \n","import pandas as pd\n","# import geopandas \n","import geopandas as gpd\n","# from geopandas import GeoDataFrame"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q8tLzJzcMh74"},"source":["### Configure Enviornment"]},{"cell_type":"code","metadata":{"id":"OuH4mBeYCUqU"},"source":["# This will just beautify the output\n","\n","pd.set_option('display.expand_frame_repr', False)\n","pd.set_option('display.precision', 2)\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# pd.set_option('display.expand_frame_repr', False)\n","# pd.set_option('display.precision', 2)\n","# pd.reset_option('max_colwidth')\n","pd.set_option('max_colwidth', 20)\n","# pd.reset_option('max_colwidth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g-eNzM614b3K"},"source":["## Prep Datasets"]},{"cell_type":"markdown","metadata":{"id":"_3QiG_W4iDl7"},"source":["### TPOP CSA and Baltimore"]},{"cell_type":"markdown","metadata":{"id":"tlQvkkbaB0ZI"},"source":["Get Baltimore"]},{"cell_type":"code","metadata":{"id":"-xeV9WHdOhBv"},"source":["#collapse_output\n","#collapse_input\n","csa = \"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Tpop/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\"\n","csa = gpd.read_file(csa);\n","csa.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QyBS5PlHB1db"},"source":["Get CSA"]},{"cell_type":"code","metadata":{"id":"2FLpVhiPUAck"},"source":["url2 = \"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/Tpop/FeatureServer/1/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\"\n","csa2 = gpd.read_file(url2);\n","csa2['CSA2010'] = csa2['City_1'] \n","csa2['OBJECTID'] = 56 \n","csa2 = csa2.drop(columns=['City_1'])\n","csa2.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RiT-Pc1jgZJI"},"source":["Append do no append Bcity. We put it on the Bottom of the df because when performing the ponp it returns only the last matching columns CSA Label. "]},{"cell_type":"code","metadata":{"id":"RWZigszHUWm5"},"source":["# csa = pd.concat([csa2, csa], ignore_index=True)\n","csa = csa.append(csa2).reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oH6C5OecjBsy"},"source":["csa.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WLpXYhJHB_rt"},"source":["csa.tail(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpZDcRVkk9SJ"},"source":["csa.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wG_g3sp2J2pq"},"source":["csa.drop(columns=['Shape__Area', 'Shape__Length', 'OBJECTID'], axis=1).to_file(\"BCity_and_CSA.geojson\", driver='GeoJSON')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PWjbHbRSL3Mp"},"source":["## Labeled Points - Analysis."]},{"cell_type":"code","metadata":{"id":"QkeIfnmkw9v5"},"source":["original = gpd.read_file(\"RBIntel_20\"+year+\"_BaltRegion_newfields_CSA_City.shp\"); "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cz7Pr7qFoT-8"},"source":["original.rename(columns={ 'CSA':'CSA2010', 'BaltCity':'InBaltimore'}, inplace=True)\n","df = original[ original['CSA2010'].notnull() | original['InBaltimore'].notnull()  ]\n","print( 'Total # Rows: ', original.shape[0] ) # rows, columns\n","print( '# Before | After')\n","print( '# Where BCity.isnull/notnull: ', original.InBaltimore.isnull().sum(), '|', original.InBaltimore.notnull().sum() ); \n","print( '# where CSA2010.isnull/notnull: ', original.CSA2010.isnull().sum(), '|', original.CSA2010.notnull().sum() ); \n","print( '# Where CSA and/or Baltimore lbl Exists: ', df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wyg1tJ01i5H4"},"source":["df.describe().to_csv('18_UnfilteredOnForeDescriptions.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DIfqD-fnjJzv"},"source":["df[ df['Foreclosur'].str.contains('.Y.|.Y|Y.|Y', regex=True, na=False) ].describe().to_csv('18_filteredOnForeDescriptions.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhLCmcq2jwkP"},"source":["df['count'] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a31fQjmVv069"},"source":["df.groupby('CSA2010').sum(numeric_only=True)['count'].to_csv('18_UnfilteredOnFore.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0QelFycjgDS"},"source":["df[ df['Foreclosur'].str.contains('.Y.|.Y|Y.|Y', regex=True, na=False) ].groupby('CSA2010').sum(numeric_only=True)['count'].to_csv('18_FilteredOnFore.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7o5yafMxcRD"},"source":["# from VitalSigns.utils import * \n","# df = check_labels(original)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gjTdZWcrmIUy"},"source":["%%capture\n","df.CSA2010 = df.CSA2010.fillna('Baltimore City')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnyTLE3dZIXH"},"source":["df.CSA2010.unique() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PzhU6cdIVqhn"},"source":["You need to run the indicator functions at least once before running this. Also just doesn't work atm."]},{"cell_type":"markdown","metadata":{"id":"k-BLJbiwWTQ1"},"source":["## VS Indicator Functions"]},{"cell_type":"markdown","metadata":{"id":"gyazzTN-BokK"},"source":["#### Preview"]},{"cell_type":"code","metadata":{"id":"yoUoj8qUbFu_"},"source":["original.shape[0]\n","print(' ')\n","df.shape[0]\n","print(' ')\n","df[ df['DaysonMark'] > 0].shape[0]\n","print(' ')\n","df[ df['Foreclosur'].str.contains('.Y.|.Y|Y.|Y', regex=True, na=False) ].shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_GSmTRHHPy9"},"source":["NewTrust1L is dead. long live BuyerFinan"]},{"cell_type":"code","metadata":{"id":"mAPItjkToVft"},"source":["df['Foreclosur'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEn-dyS2HQsX"},"source":["df['BuyerFinan'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NiS0UtkH2gBg"},"source":["df[ df['DaysonMark'] > 0].shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RLbq8HHBjM8i"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Il4KHFPLpZ0s"},"source":["df[ df['DaysonMark'] > 0][['CSA2010','DaysonMark']].groupby('CSA2010').agg(['median', 'count']).head(4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFtMjkldXCCY"},"source":["#### DOM 30\n"]},{"cell_type":"code","metadata":{"id":"RpNjaKjsC51T"},"source":["# https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/dom/FeatureServer/layers\n","# https://bniajfi.org/indicators/Housing%20And%20Community%20Development/dom\n","\n","lbl = 'Median Number of Days on the Market'\n","TopicArea = 'Housing And Community Development'\n","YearsAvailable = '2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020'\n","long_Description: \"\"\"\n","  The median number of days that homes listed for sale sit on the public market in a given area. \n","  This time period is from the date it is listed for sale till the day the contract of sale is signed. \n","  Private (non-listed) home sale transactions are not included in this indicator. \n","  The median days on market is used as opposed to the average so that both extremely high and extremely \n","  low days on the market do not distort the length of time for which homes are listed on the market.\n","  \"\"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKfal4_nxPBz"},"source":["Used to be we'd calc our own DOM field but RBIntel added it as a field"]},{"cell_type":"code","metadata":{"id":"-Sl2C2fe6P_p"},"source":["#export\n","def vsDom(df, yr):\n","  id = '30'\n","  shortname = 'dom'\n","  fincol = id+'-'+shortname+year \n","\n","  # Create the Numerator and Denominator\n","  numer = df[['DaysonMark','CSA2010']].copy() \n","\n","  # Filter Em\n","  numer = numer[ numer['DaysonMark'] > 0]\n","  print( numer.shape[0] )\n","\n","  # Get Bcity Val\n","  bCityVal = numer.median(numeric_only=True)['DaysonMark']\n"," \n","  # Group by CSA\n","  numer = numer.groupby('CSA2010').median(numeric_only=True) # use .median to calculate DOM.\n","\n","  # Make sure ALL csas and BaltimoreCity are included and sorted.\n","  numer = csa.merge( numer, left_on='CSA2010', right_on='CSA2010', how='outer' ) \n","  numer.drop( columns=['geometry', 'Shape__Length','Shape__Area', 'OBJECTID', 'tpop10'], inplace=True)\n","\n","  # Bcity is the median of all the records an not the community medians.\n","  # Incorrect Bcity median IFF Groupby keeps a 'False' row (index 56)\n","  numer.at[55,'DaysonMark']= bCityVal\n","\n","  # Perform the calculation\n","  numer[fincol] = numer['DaysonMark']\n","  \n","  compareYears = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/\"+shortname.capitalize()+\"/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\");\n","  goback = 2 if year == '19' else 3 if year == '20' else 1\n","  prevYear = shortname + str( int(year) - goback )\n","  if prevYear in compareYears.columns:\n","    numer = numer.merge( compareYears[['CSA2010', prevYear]], left_on='CSA2010', right_on='CSA2010', how='outer' ) \n","    numer['change'] = numer[id+'-'+shortname+year] - numer[ prevYear ]\n","    numer['percentChange'] = numer['change' ] / numer[ prevYear ] * 100\n","    numer['change'] = numer['change'].apply(lambda x: \"{:.2f}\".format(x) )\n","    print( 'Records Matching Query: ', numer.size / len(numer.columns) )\n","\n","  return numer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQFWnKAfw1RT"},"source":["t = vsDom(df,year)\n","t.head(2)\n","t.tail(2)\n","t.to_csv('30_dom_'+year+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3NhGddiW81g"},"source":["#### CASHSA 38\n"]},{"cell_type":"markdown","metadata":{"id":"uYIxnYCwIvV4"},"source":["NewTrust1L is dead. long live BuyerFinan"]},{"cell_type":"code","metadata":{"id":"PIreTKxIChJi"},"source":["# https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/cashsa/FeatureServer/layers\n","# https://bniajfi.org/indicators/Housing%20And%20Community%20Development/cashsa\n","\n","lbl = 'Percentage of Residential Sales for Cash'\n","TopicArea = 'Housing And Community Development'\n","YearsAvailable = '2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018'\n","long_Description: \"\"\"\n","  The percent of homes and condominiums sold for cash out of all residential properties sold in a calendar year. \n","  These types of sales tend to signify investor-based purchases as homes purchased for cash either \n","  become rental properties or later sold again in an effort to generate a profit.\n","  \"\"\"\n","\n","original_sql =  = \"\"\"\n","  with numerator AS (\n","    select (sum(\n","    case \n","    when newtrust1l = $$Cash$$\n","    then 1\n","    else 0\n","    end)::numeric) as result, csa\n","    from vital_signs.match_csas_and_bc_by_geom('housing.rbintelregion_2017', 'gid', 'the_geom') a\n","    left join housing.rbintelregion_2017 b on a.gid = b.gid\n","    group by csa\n","    ),\n","    denominator AS (\n","      select (sum(\n","      case \n","      when csa_present\n","      then 1\n","      else NULL\n","      end)::numeric \n","      ) as result, csa\n","      from vital_signs.match_csas_and_bc_by_geom('housing.rbintelregion_2017', 'gid', 'the_geom') a\n","      left join housing.rbintelregion_2017 b on a.gid = b.gid\n","      group by csa, the_pop\n","    ),\n","    tbl AS (\n","      select denominator.csa,(numerator.result / denominator.result)*(100::numeric) as result \n","      from numerator left join denominator on numerator.csa = denominator.csa\n","      )\n","\n","  select * from tbl where 1 = 1 ORDER BY csa ASC;\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0UgdSRY7ecQ"},"source":["#export\n","def cashsa(df, yr):\n","  id = '38'\n","  shortname = 'cashsa'\n","  fincol = id+'-'+shortname+year \n","\n","  # Create the Numerator and Denominator\n","  numer = df[['BuyerFinan','CSA2010']].copy()\n","  numer['count'] = 1\n","  denom = numer.copy()\n","\n","  # Filter Em  \n","  numer = numer[ numer['BuyerFinan'].str.contains('.Cash.|.Cash|Cash.|Cash', regex=True, na=False) ] \n","  print(\"LENGTH AFTER FILTER: \", len(numer) )\n","\n","  # Get Bcity Val\n","  bCityVal = numer.sum(numeric_only=True)['count']\n","  bCityValDenom = denom.sum(numeric_only=True)['count']\n","\n","  # Group by CSA\n","  numer = numer.groupby('CSA2010').sum(numeric_only=True)\n","  denom = denom.groupby('CSA2010').sum(numeric_only=True) \n","  \n","  # Make sure ALL csas and BaltimoreCity are included and sorted.\n","  numer = csa.merge( numer, left_on='CSA2010', right_on='CSA2010', how='outer' ) \n","  numer.drop( columns=['geometry', 'Shape__Length','Shape__Area', 'OBJECTID', 'tpop10'], inplace=True)\n","\n","  denom = csa.merge( denom, left_on='CSA2010', right_on='CSA2010', how='outer' ) \n","  denom.drop( columns=['geometry', 'Shape__Length','Shape__Area', 'OBJECTID', 'tpop10'], inplace=True)\n","\n","  # Bcity is the sum of the community sums.\n","  # Incorrect Bcity Sum IFF Groupby keeps a 'False' row (index 56)\n","  numer.at[55,'count']= bCityVal\n","  denom.at[55,'count']= bCityValDenom\n","\n","  # Perform the calculation \n","  numer[fincol] = numer['count'] / denom['count'] * 100\n","\n","  compareYears = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/\"+shortname.capitalize()+\"/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\");\n","  goback = 2 if year == '19' else 3 if year == '20' else 1\n","  prevYear = shortname + str( int(year) - goback )\n","  if prevYear in compareYears.columns:\n","    numer = numer.merge( compareYears[['CSA2010', prevYear]], left_on='CSA2010', right_on='CSA2010', how='outer' ) \n","    numer['change'] = numer[id+'-'+shortname+year] - numer[ prevYear ]\n","    numer['percentChange'] = numer['change' ] / numer[ prevYear ] * 100\n","    numer['change'] = numer['change'].apply(lambda x: \"{:.2f}\".format(x) )\n","    print( 'Records Matching Query: ', numer.size / len(numer.columns) )\n","\n","  return numer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWuW0QoBw3yh"},"source":["resp = cashsa(df, year)\n","resp.head(2)\n","resp.tail(2)\n","cashsa(df,year).to_csv('38_cashsa_'+year+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BY82-XOAW14Z"},"source":["#### REOSA 39"]},{"cell_type":"code","metadata":{"id":"QQuq4iKTCaWz"},"source":["# https://services1.arcgis.com/mVFRs7NF4iFitgbY/arcgis/rest/services/reosa/FeatureServer/layers\n","# https://bniajfi.org/indicators/Housing%20And%20Community%20Development/reosa/2017\n","\n","lbl = 'Percentage of Residential Sales in Foreclosure (REO)'\n","TopicArea = 'Housing And Community Development'\n","YearsAvailable = '2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020'\n","long_Description: \"\"\"\n","  The portion of the homes and condominiums sold that were identified as \n","  being owned by the bank (REO) out of all residential properties sold in a calendar year.\n","  \"\"\"\n","\n","original_sql = \"\"\"\n","  with numerator AS (\n","   select (sum(\n","   case \n","   when foreclosur = $$Y$$\n","   then 1\n","   else 0\n","   end)::numeric) as result, csa\n","   from vital_signs.match_csas_and_bc_by_geom('housing.rbintelregion_2017', 'gid', 'the_geom') a\n","   left join housing.rbintelregion_2017 b on a.gid = b.gid\n","   group by csa\n","   ),\n","   denominator AS (\n","    select (sum(\n","     case \n","     when csa_present\n","     then 1\n","     else NULL\n","     end)::numeric \n","    ) as result, csa\n","    from vital_signs.match_csas_and_bc_by_geom('housing.rbintelregion_2017', 'gid', 'the_geom') a\n","    left join housing.rbintelregion_2016 b on a.gid = b.gid\n","    group by csa, the_pop\n","   ),\n","   tbl AS (\n","     select denominator.csa,(numerator.result / denominator.result)*(100::numeric) as result \n","     from numerator left join denominator on numerator.csa = denominator.csa\n","    )         select * from tbl where 1 = 1 ORDER BY csa ASC;\n","  \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXqiKDwl-88v"},"source":["#export\n","def reosa(df, yr):\n","  id = '39'\n","  shortname = 'reosa'\n","  fincol = id+'-'+shortname+year \n","\n","  # Create the Numerator and Denominator\n","  numer = df[['Foreclosur','CSA2010']].copy() \n","  numer['count'] = 1 \n","  \n","  denom = numer.copy() \n","  denom['count'] = 1 \n","  \n","  # Filter Em\n","  numer = numer[ numer['Foreclosur'].str.contains('.Y.|.Y|Y.|Y', regex=True, na=False) ] \n","  print( numer['Foreclosur'].value_counts() )\n","\n","  # Get Bcity Val\n","  bCityVal = numer.sum(numeric_only=True)['count']\n","  bCityValDenom = denom.sum(numeric_only=True)['count']\n"," \n","  # Group by CSA\n","  numer = numer.groupby('CSA2010').sum(numeric_only=True) \n","  denom = denom.groupby('CSA2010').sum(numeric_only=True) \n","   \n","  # Make sure ALL csas and BaltimoreCity are included and sorted.\n","  numer = csa.merge( numer, left_on='CSA2010', right_on='CSA2010', how='outer' ) \n","  numer.drop( columns=['geometry', 'Shape__Length','Shape__Area', 'OBJECTID', 'tpop10'], inplace=True)\n","\n","  denom = csa.merge( denom, left_on='CSA2010', right_on='CSA2010', how='outer' ) \n","  denom.drop( columns=['geometry', 'Shape__Length','Shape__Area', 'OBJECTID', 'tpop10'], inplace=True)\n","\n","  # Bcity is the sum of the community sums.\n","  # Incorrect Bcity Sum IFF Groupby keeps a 'False' row (index 56)\n","  numer.at[55,'count']= bCityVal\n","  denom.at[55,'count']= bCityValDenom\n","\n","  # Perform the calculation \n","  numer['denomCount'] = denom['count']\n","  numer[fincol] = numer['count'] / numer['denomCount'] * 100\n","  \n","  compareYears = gpd.read_file(\"https://services1.arcgis.com/mVFRs7NF4iFitgbY/ArcGIS/rest/services/\"+shortname.capitalize()+\"/FeatureServer/0/query?where=1%3D1&outFields=*&returnGeometry=true&f=pgeojson\");\n","  goback = 2 if year == '19' else 3 if year == '20' else 1\n","  prevYear = shortname + str( int(year) - goback )\n","  if prevYear in compareYears.columns:\n","    numer = numer.merge( compareYears[['CSA2010', prevYear]], left_on='CSA2010', right_on='CSA2010', how='outer' ) \n","    numer['change'] = numer[id+'-'+shortname+year] - numer[ prevYear ]\n","    numer['percentChange'] = numer['change' ] / numer[ prevYear ] * 100\n","    numer['change'] = numer['change'].apply(lambda x: \"{:.2f}\".format(x) )\n","    print( 'Records Matching Query: ', numer.size / len(numer.columns) )\n","\n","  return numer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1sz2k3O-w5so"},"source":["resp = reosa(df, year)\n","resp.head()\n","resp.tail()\n","resp.to_csv('39_reosa_'+year+'.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bq3shPVnj0n0"},"source":["## OLD_FNS"]},{"cell_type":"code","metadata":{"id":"ig0qFxGmFtt-"},"source":["# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html\n","# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html\n","# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n","# https://stackoverflow.com/questions/41308763/python-pandas-df-duplicated-and-df-drop-duplicated-not-finding-all-duplicates\n","def exploreDs(df, yr):\n","\n","  def createIndicatorAndPlotChoropleth(ddf, txt1):\n","    fig, ax = plt.subplots(1, 1)\n","    csa.merge( vsDom(df, 'DOM_'+txt1+yr) , left_on='CSA2010', right_on='CSA2010' ).plot(column='dom', ax=ax, legend=True); plt.savefig('./output/img/DOM_Map_Of_the_'+txt1+yr+'.jpg')\n","    csa.merge( vsCashsa(df, 'Cashsa_'+txt1+yr) , left_on='CSA2010', right_on='CSA2010' ).plot(column='cashsa', ax=ax, legend=True); plt.savefig('./output/img/Cashsa_Map_Of_the_'+txt1+yr+'.jpg')\n","    csa.merge( vsReosa(df, 'Reosa_'+txt1+yr) , left_on='CSA2010', right_on='CSA2010' ).plot(column='reosa', ax=ax, legend=True); plt.savefig('./output/img/Reosa_Map_Of_the_'+txt1+yr+'.jpg')\n","\n","  def plotAndSave(ddf, txt):\n","    fig, ax = plt.subplots(1, 1)\n","    base = csa.plot(color='white', edgecolor='black')\n","    ddf.plot(ax=base, marker='o', color='green', markersize=5);\n","    plt.savefig('./output/'+txt)\n","\n","  print('!~~~~~~~~~~~~~~~~~~~~~!STARTING!!!!!! ',yr,' !~~~~~~~~~~~~~~~~~~~~~!')\n","\n","  #\n","  # Drop All un-needed Columns\n","  df = df[['CSA2010', 'AddressLin', 'geometry', 'DaysOnMark', 'NewTrust1L', 'Foreclosur', 'SoldDate']]\n","  # Sort the Dataset by Address\n","  #\n","  df = df.sort_values(by=['AddressLin']).reset_index()\n","  print('Given: ', len(df), ' Records')\n","  # Run this Indicators\n","  createIndicatorAndPlotChoropleth(df, 'Untouched_Records')\n","  # Plot it on a CSA Map\n","  plotAndSave(df, 'Dot_Map_Of_the_Untouched_Records_'+yr+'.jpg') \n","\n","  #\n","  # Drop the NON CSA Records\n","  # Save a copy of the Dropped Records? \n","  # - Nah. They wont effect our calculations and removing them adds clarity.\n","  #\n","  df.drop(df[df['CSA2010'] == 'false'].index, inplace=True)\n","  print('There are ', len(df), ' Records Remaining after Droping Non-CSA Records')\n","  # Run this Indicators\n","  createIndicatorAndPlotChoropleth(df, 'Dropped_Non_CSA_Records')\n","  # Plot it on a CSA Map\n","  plotAndSave(df, 'Dot_Map_Of_the_Dropped_Non_CSA_Records_'+yr+'.jpg') \n","\n","  #\n","  # Determines which duplicates (if any) to keep. \n","  # - first : Drop duplicates except for the first occurrence. \n","  # - last : Drop duplicates except for the last occurrence. \n","  # - False : Drop all duplicates.\n","  # Filter the dataset for duplicates in the AddressLin.\n","  #\n","  val1 = df.drop_duplicates(subset=['SoldDate', 'AddressLin'], keep='last').reset_index()\n","  print('There are', len(val1) , ' Records Remaining after Droping all but the last Duplicate on SoldDate & AddressLin')\n","  # Run this Indicators\n","  createIndicatorAndPlotChoropleth(val1, 'Dropped_Non_CSA_Records_and_Deduped')\n","  # Plot it on a CSA Map\n","  plotAndSave(val1, 'Dot_Map_Of_the_Dropped_Non_CSA_Records_and_Deduped_'+yr+'.jpg') \n","  \n","  #\n","  # Save a copy of the data that was filtered out in a new dataset\n","  #\n","  val2 = df[df.duplicated(subset=['SoldDate', 'AddressLin'], keep=False)].reset_index()\n","  print('Having Removed This Many: ', len(val2))\n","  # Run this Indicators\n","  createIndicatorAndPlotChoropleth(val2, 'Dropped_Non_CSA_Records_and_Kept_Only_Duplicates')\n","  # Plot it on a CSA Map\n","  plotAndSave(val2, 'Dot_Map_Of_the_Dropped_Non_CSA_Records_and_Kept_Only_Duplicates_'+yr+'.jpg') \n","\n","  return ( val1, val2, df )\n","\n","# r177, val217, val317 = exploreDs(r17, '17')\n","# r188, val218, val318 = exploreDs(r18, '18')\n","# r189, val219, val319 = exploreDs(df, year)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3e_foRXPj1Hr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HhoIe2N9RSgY"},"source":["def inspectReosaDenominator(df, yr):\n","  print( 'Unique Foreclosure Values', df.Foreclosur.unique() )\n","  print( 'Unique NewTrust1L  Values', df.NewTrust1L.unique() )\n","  # Dedupe on 'AddressLin', 'SoldDate\n","  print( \"Original Dataset's Length: \", len(df))\n","  temp = df.drop_duplicates(subset=['AddressLin', 'SoldDate'], keep='last')\n","  print('Deduped Length: ', len(temp))\n","  print('Numer of Records Removed: ', len(df) - len(temp))\n","  # Drop any NA AddressLin\n","  temp = temp.dropna(subset=['AddressLin'])\n","  print('Num Removed With No NA Addresses: ', len(df) - len(temp))\n","\n","  temp.head(1) # CSA2010 AddressLin SoldDate\n","  temp['count'] = 1\n","  v1= temp.groupby(by=[\"CSA2010\",\"Foreclosur\"]).sum()\n","  v2= temp.groupby(by=[\"CSA2010\",\"DaysOnMark\"]).median()\n","  v3= temp.groupby(by=[\"CSA2010\",\"NewTrust1L\"]).sum() # .sort_values(by=['col1', 'col2'])\n","  v1.to_csv('reosa_Deduped'+yr+'_CSAs_Unique_Foreclosure_Counts.csv', index=False)\n","  v2.to_csv('reosa_Deduped'+yr+'_CSAs_Unique_DOM_Counts.csv', index=False) \n","  v3.to_csv('reosa_Deduped'+yr+'_CSAs_Unique_CASHSA_Counts.csv', index=False) \n","  return temp\n","\n","# inspectReosaDenominator(df, year)  \n","# Compare DS's for each CSA where Points Exists but A ForeClosure Value Does not."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBlO9_s198_g"},"source":["def retrieveAndcleanRbIntel(filename, year):\n","  rbintel = gpd.read_file(filename);\n","  print(len(rbintel));\n","  # Convert to EPSG:4326\n","  rbintel = rbintel.to_crs(epsg=4326)\n","  rbintel.crs\n","\n","  rbintel['x'] = rbintel.geometry.x\n","  rbintel['y'] = rbintel.geometry.y\n","\n","  # Reference: All Points\n","  base = csa.plot(color='white', edgecolor='black')\n","  rbintel.plot(ax=base, marker='o', color='green', markersize=5);\n","\n","  # Get CSA Labels for all Points.\n","  rbintelCSA = workWithGeometryData( \n","      method='ponp', df=rbintel, polys=csa, ptsCoordCol='geometry', \n","      polygonsCoordCol='geometry', polyColorCol=False, polygonsLabel='CSA2010'\n","  )\n","  rbintelCSA = rbintelCSA.drop('geometry',axis=1)\n","  rbintelCSA.to_csv('ponp_rbintel_'+year+'.csv', index=False)\n","  return rbintelCSA"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJnN5wBbeudH"},"source":["Region17 and Region 18 should have a similar number of records"]}]}